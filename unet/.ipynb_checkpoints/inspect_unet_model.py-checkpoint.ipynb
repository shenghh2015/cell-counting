{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from skimage import io\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import segmentation_models_v1 as sm\n",
    "import albumentations as A\n",
    "sm.set_framework('tf.keras')\n",
    "from unet_model import U_Net\n",
    "\n",
    "from helper_function import plot_history, generate_folder\n",
    "from helper_function import precision, recall, f1_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet-epoch-5000-batch-16-lr-0.0001-dim-256-set-bone_marrow-loss-bce-cross-1\n",
      "dataset:bone_marrow\n",
      "cross:1\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "docker = False\n",
    "model_dir = './models/unet'\n",
    "# model_name = 'regnet-epoch-5000-batch-16-lr-0.0001-dim-256-set-hESC-loss-mse-cross-1'\n",
    "# model_name = 'regnet-epoch-5000-batch-16-lr-0.0001-dim-256-set-colorectal-loss-mse-cross-1'\n",
    "#model_name = 'regnet-epoch-5000-batch-16-lr-0.0001-dim-256-set-bone_marrow-loss-mse-cross-1'\n",
    "#model_name = 'regnet-epoch-5000-batch-64-lr-0.0001-dim-128-set-bone_marrow-loss-mse-cross-1'\n",
    "model_names = os.listdir(model_dir)\n",
    "model_name = model_names[0]\n",
    "splits = model_name.split('-')\n",
    "for v in range(len(splits)):\n",
    "    if splits[v] == 'set':\n",
    "        dataset = splits[v+1]\n",
    "    elif splits[v] == 'cross':\n",
    "        cross = int(splits[v+1])\n",
    "print(model_name); print('dataset:{}'.format(dataset)); print('cross:{}'.format(cross))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/unet/bone_marrow/cross-1/val/pr_masks\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'bacterial':\n",
    "\tval_dim = 256; img_dim = 256\n",
    "elif dataset == 'bone_marrow':\n",
    "\tval_dim = 608; img_dim = 600\n",
    "elif dataset == 'colorectal':\n",
    "\tval_dim = 512; img_dim = 500\n",
    "elif dataset == 'hESC':\n",
    "\tval_dim = 512; img_dim = 512\n",
    "\n",
    "DATA_DIR = '/data/datasets/unet/{}'.format(dataset) if docker else './datasets/unet/{}'.format(dataset)\n",
    "DATA_DIR = DATA_DIR+'/cross-{}'.format(cross)\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val', 'images')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'val', 'masks')\n",
    "pred_valid_dir = os.path.join(DATA_DIR, 'val', 'pr_masks'); print(pred_valid_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['bk', 'cell']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None,\n",
    "            nb_data=None,\n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        id_list = os.listdir(images_dir)\n",
    "        if nb_data ==None:\n",
    "            self.ids = id_list\n",
    "        else:\n",
    "            self.ids = id_list[:int(min(nb_data,len(id_list)))]\n",
    "        #self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        #print(self.images_fps[:4]); print(self.masks_fps[:4])\n",
    "        print(len(self.images_fps)); print(len(self.masks_fps))\n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "#         print(np.unique(mask))\n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "#         print(self.class_values)\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # add background if mask is not binary\n",
    "        if mask.shape[-1] != 1:\n",
    "            background = 1 - mask.sum(axis=-1, keepdims=True)\n",
    "            mask = np.concatenate((mask, background), axis=-1)\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset and form batches\n",
    "    \n",
    "    Args:\n",
    "        dataset: instance of Dataset class for image loading and preprocessing.\n",
    "        batch_size: Integet number of images in batch.\n",
    "        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(dataset))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "        \n",
    "        # transpose list of lists\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "#         map_batch = batch[1]\n",
    "#         map_batch_list = [map_batch]\n",
    "#         for i in range(4):\n",
    "#             map_batch_list.append(map_batch[:,::2,::2,:])\n",
    "#             map_batch = map_batch[:,::2,::2,:]\n",
    "#         map_batch_list.reverse()\n",
    "#         map_tuple = ()\n",
    "#         for i in range(5):\n",
    "#             map_tuple = map_tuple+(map_batch_list[i],)\n",
    "        return (batch[0], batch[1])\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.indexes) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)\n",
    "\n",
    "def get_validation_augmentation(dim = 256):\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        A.PadIfNeeded(dim, dim),\n",
    "        A.RandomCrop(height=dim, width=dim, always_apply=True)\n",
    "#         A.PadIfNeeded(384, 480)\n",
    "    ]\n",
    "    return A.Compose(test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sh38/anaconda3/envs/proGAN_tf/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "./models/unet/unet-epoch-5000-batch-16-lr-0.0001-dim-256-set-bone_marrow-loss-bce-cross-1/best_model.h5\n"
     ]
    }
   ],
   "source": [
    "CLASSES = ['cell']\n",
    "n_classes = 1 \n",
    "#create model\n",
    "net_type = 'U_Net'\n",
    "net_func = globals()[net_type]\n",
    "model = net_func(None, None, color_type = 3, num_class =n_classes)\n",
    "model_folder = '/data/models/unet/{}'.format(model_name) if docker else './models/unet/{}'.format(model_name)\n",
    "model_file = model_folder+'/best_model.h5'\n",
    "print(model_file)\n",
    "model.load_weights(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv11_1 (Conv2D)               (None, None, None, 3 896         main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv11_2 (Conv2D)               (None, None, None, 3 9248        conv11_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, None, None, 3 0           conv11_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv21_1 (Conv2D)               (None, None, None, 6 18496       pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv21_2 (Conv2D)               (None, None, None, 6 36928       conv21_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2 (MaxPooling2D)            (None, None, None, 6 0           conv21_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv31_1 (Conv2D)               (None, None, None, 1 73856       pool2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv31_2 (Conv2D)               (None, None, None, 1 147584      conv31_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3 (MaxPooling2D)            (None, None, None, 1 0           conv31_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv41_1 (Conv2D)               (None, None, None, 2 295168      pool3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv41_2 (Conv2D)               (None, None, None, 2 590080      conv41_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4 (MaxPooling2D)            (None, None, None, 2 0           conv41_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv51_1 (Conv2D)               (None, None, None, 5 1180160     pool4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv51_2 (Conv2D)               (None, None, None, 5 2359808     conv51_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up42 (Conv2DTranspose)          (None, None, None, 2 524544      conv51_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge42 (Concatenate)           (None, None, None, 5 0           up42[0][0]                       \n",
      "                                                                 conv41_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv42_1 (Conv2D)               (None, None, None, 2 1179904     merge42[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv42_2 (Conv2D)               (None, None, None, 2 590080      conv42_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up33 (Conv2DTranspose)          (None, None, None, 1 131200      conv42_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge33 (Concatenate)           (None, None, None, 2 0           up33[0][0]                       \n",
      "                                                                 conv31_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv33_1 (Conv2D)               (None, None, None, 1 295040      merge33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv33_2 (Conv2D)               (None, None, None, 1 147584      conv33_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up24 (Conv2DTranspose)          (None, None, None, 6 32832       conv33_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge24 (Concatenate)           (None, None, None, 1 0           up24[0][0]                       \n",
      "                                                                 conv21_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv24_1 (Conv2D)               (None, None, None, 6 73792       merge24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv24_2 (Conv2D)               (None, None, None, 6 36928       conv24_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up15 (Conv2DTranspose)          (None, None, None, 3 8224        conv24_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge15 (Concatenate)           (None, None, None, 6 0           up15[0][0]                       \n",
      "                                                                 conv11_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv15_1 (Conv2D)               (None, None, None, 3 18464       merge15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv15_2 (Conv2D)               (None, None, None, 3 9248        conv15_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output (Conv2D)                 (None, None, None, 1 33          conv15_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,760,097\n",
      "Trainable params: 7,760,097\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "test_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    classes = CLASSES,\n",
    "    augmentation=get_validation_augmentation(val_dim),\n",
    "    preprocessing=None\n",
    ")\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=1, shuffle=False)\n",
    "pr_masks = model.predict(test_dataloader);pr_masks= pr_masks[:,:,:,0].squeeze()\n",
    "images = []; gt_masks = []\n",
    "for i in range(len(test_dataset)):\n",
    "    image, gt_mask = test_dataset[i]\n",
    "    images.append(image); gt_masks.append(gt_mask[:,:,:,0])\n",
    "images = np.stack(images)\n",
    "gt_masks = np.stack(gt_masks).squeeze() #gt_maps = gt_masks.squeeze()\n",
    "print(gt_masks.max())\n",
    "\n",
    "# crop \n",
    "if dataset == 'bone_marrow' or dataset == 'colorectal':\n",
    "\toffset1, offset2 = int((val_dim-img_dim)/2), val_dim-int((val_dim-img_dim)/2)\n",
    "\tgt_masks=gt_masks[:,offset1:offset2,offset1:offset2]\n",
    "\tpr_masks=pr_masks[:,offset1:offset2,offset1:offset2]\n",
    "\timages=images[:,offset1:offset2,offset1:offset2]\n",
    "\tprint('output: {}'.format(pr_masks.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
